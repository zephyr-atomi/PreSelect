# 试点计划：应用PreSelect方法筛选Function Calling数据

本文档是一个具体的、可执行的试点计划，旨在通过 **函数调用 (Function Calling)** 这一个任务，来端到端地验证 PreSelect 方法论的有效性。计划成功后，可将此流程推广至其他任务。

## 目标

用最小的成本和资源，走通从"创建探针模型"到"产出专用数据集"再到"评测效果"的完整流程，并量化证明我们筛选出的数据能够显著提升模型在 Function Calling 任务上的表现。

---

## 阶段一：准备工作与信号收集

### 步骤 1.1：创建简化的"探针模型"集合

我们不需要完整的 Model Zoo，只需要制造出与 Function Calling 能力相关的"差异性"即可。

-   **核心方法**：以比赛提供的 **起始模型 (Starter Model)** 为基础，使用少量外部数据对其进行短暂微调。

-   **微调数据量与步数 (重要！)**:
    -   **数据量**: 每个微调任务使用几千到几万个高质量样本即可，重点是数据的领域纯度。
    -   **步数**: 约100-500个优化步数是一个好的起点。
    -   **风险与权衡**:
        -   **微调不足**: 如果训练步数太少或数据太少，探针模型与基础模型差异过小，会导致它们的Loss值非常接近。这会使后续计算出的"预测力分数"失去区分度，都聚集在0附近。
        -   **微调过度**: 如果训练太久，模型可能过拟合，变成一个"狭隘的专家"，失去泛化能力，同样会干扰信号。
    -   **建议**: 在大规模计算BPC前，可以先在少量（如1000个）文档上进行"理智检查"，看看不同探针模型的Loss分布是否已经拉开了差距。

-   **建议的简化模型列表**：
    1.  **模型0：基础模型 (Baseline Model)**
        -   **来源**: 直接使用官方 **Starter Model**。
        -   **目的**: 作为能力零点，是衡量效果提升的基准。

    2.  **模型A：代码与逻辑推理模型 (Code & Logic Model)**
        -   **来源**: 使用 [The Stack (small)](https://huggingface.co/datasets/bigcode/the-stack-smol) 的 Python/Java 子集微调 Starter Model。
        -   **目的**: Function Calling 本质上是结构化文本的生成，与代码的结构和逻辑相似。这个模型提供了一个"泛化相关"的能力维度。

    3.  **模型B：函数调用专属模型 (Dedicated FC Model)**
        -   **来源**: 使用 [Glaive Function Calling v2](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2) 微调 Starter Model。
        -   **目的**: 建立一个在此项任务上的"专家"模型，其 Loss 对于包含函数调用意图的文本会非常敏感。

### 步骤 1.2：准备候选数据集

-   **样本量策略 (成本与效果的权衡)**:
    -   **主要成本**: 真正的计算瓶颈在于步骤 1.4 的 BPC 计算。对百万级样本用多个模型计算BPC，成本极高。
    -   **建议起点**: 从 **10万至20万 (100k - 200k)** 的随机样本开始。这个数量对于训练一个有效的 fastText 分类器通常已经足够，并且计算成本相对可控。
    -   **验证与迭代**: 在用这批样本计算出"预测力分数"后，绘制一个直方图观察其分布。如果信号足够强（分数分布广泛，甚至呈双峰），则可继续；如果信号微弱（分数拥挤在一起），再考虑增加样本量到50万或更多，以增强信号。

-   **方法**：从比赛提供的完整 **起始数据集 (Starter Dataset)** 中，随机抽取建议数量的样本作为分析对象。

### 步骤 1.3：评测 Function Calling 任务表现

-   **评测工具**: 使用官方发布的验证集 [**data4elm/ELMB-FunctionCalling**](https://huggingface.co/datasets/data4elm/ELMB-FunctionCalling)。
-   **方法**：
    1.  对上述三个模型（基础模型、代码模型、FC模型）进行评测。
    2.  记录下它们各自的得分，得到一个分数向量：`FC_scores = [score_model_base, score_model_A, score_model_B]`。

### 步骤 1.3.5：验证探针模型 (关键质量控制点)

在投入大规模计算资源之前，必须验证探针模型是否构建成功。验证需通过"宏观"和"微观"两项检查。

-   **1. 宏观检查 (基于任务表现)**:
    -   **目的**: 确保模型们学习到了**不同的能力**。
    -   **操作**: 复用步骤 1.3 的结果，即 `FC_scores` 向量。
    -   **成功标准**: 分数向量**必须表现出显著的差异性**。例如，`[0.50, 0.68, 0.85]` 是一个理想的结果，表明模型能力已成功拉开差距。而 `[0.50, 0.51, 0.52]` 则是失败的信号。
    -   **失败对策**: 回到步骤 1.1，增加微调步数或使用更有效的微调数据。

-   **2. 微观检查 (基于损失分布)**:
    -   **目的**: 确保模型们形成了**不同的文本偏好**。
    -   **操作**: 创建一个包含代码、函数调用、通用文本等多种类型的手动"诊断集"（约10-20篇文档），仅在此诊断集上计算BPC。
    -   **成功标准**: BPC损失值必须符合逻辑预期。例如，代码模型在代码样本上损失最低；函数调用模型在函数调用样本上损失最低；而在通用文本上，专业模型的损失不应低于（甚至可能高于）基础模型。
    -   **失败对策**: 如果BPC值混乱无逻辑，说明微调未能注入预期的"偏见"。需重新审视微调过程和数据。

### 步骤 1.4：计算每个文档的损失（Loss/BPC）

-   **方法**: 对于上述三个模型，计算它们在"候选数据集"中**每一篇文档**上的损失（Loss）。最终产出一个 `[3 x 文档数量]` 大小的损失值矩阵。
-   **脚本**: `data_processing/bpc/main.py`

-   **"损失"到底是什么 (BPC/Perplexity)？**
    -   这里的"损失"具体指的是 **BPC (Bits Per Character)**，它衡量了模型压缩一段文本的能力，可以理解为模型对文本的"困惑程度"。**BPC 值越低，说明模型对这段文本的预测越准，文本内容与模型的"知识体系"越契合。**
    -   **计算过程**: 
        1.  模型在不进行权重更新（即推理模式）的情况下，逐个 token 地阅读文档。
        2.  对于每个 token，模型根据上文预测下一个 token 的概率分布。
        3.  我们取出真实 token 对应的概率，通过 `-log(probability)` 计算出该位置的交叉熵损失 (Cross-Entropy Loss)。
        4.  将整篇文档所有 token 的损失相加，然后除以文档的总字符数（或总字节数），再进行一个单位换算（除以`log(2)`），就得到了 BPC 值。
    -   这个过程本质上是在问："平均来看，我需要多少个比特（bits）的信息，才能编码这篇文档中的每一个字符？"

---

## 阶段二：训练 Function Calling 分类器

### 步骤 2.1：计算"预测力分数"

-   **方法**：
    1.  遍历"候选数据集"中的每一篇文档。
    2.  对每一篇文档，计算其**损失向量** `[loss_model_base, loss_model_A, loss_model_B]` 与 **任务表现向量** `FC_scores` 之间的相关性（皮尔逊或斯皮尔曼相关系数）。
    3.  这个相关系数就是该文档对 Function Calling 任务的"预测力分数"。

### 步骤 2.2 & 2.3：准备数据并训练分类器

-   **方法**：
    1.  根据上一步的"预测力分数"对文档进行排序。
    2.  将分数最高的前 N% （例如，前20%）的文档标记为 `__label__1`，其余标记为 `__label__0`，生成 fastText 训练文件。
    3.  使用 `data_processing/fasttext/train_fasttext.py` 训练模型，产出 `function_calling_classifier.bin`。

---

## 阶段三：筛选、训练与效果验证

### 步骤 3.1：筛选 Function Calling 数据集

-   **方法**: 使用训练好的 `function_calling_classifier.bin`，对**全部**的 **Starter Dataset** 进行筛选，将所有被判定为 `__label__1` 的文档保存下来，形成一个高质量的、专用于 Function Calling 的数据集。

### 步骤 3.2：训练最终模型

-   **方法**: 以官方 **Starter Model** 为基础，使用上一步筛选出的 Function Calling 数据集对其进行持续预训练，得到一个新模型 `Improved_FC_Model`。

### 步骤 3.3：验证试点效果

-   **这是整个试点计划最关键的一步，用于证明方法的有效性。**
-   **方法**:
    1.  使用官方验证集 `data4elm/ELMB-FunctionCalling` 评测新模型 `Improved_FC_Model` 的分数，记为 `S_improve`。
    2.  获取原始 **Starter Model** 在该验证集上的分数 `S_base` (这个分数在步骤1.3中已经得到)。
    3.  计算性能提升 `S = S_improve - S_base`。

-   **成功标准**: 如果 `S` 是一个显著的正数，则证明我们的 PreSelect 方法是有效的，筛选出的数据确实对提升 Function Calling 能力有帮助。 